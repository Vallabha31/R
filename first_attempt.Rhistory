library(factoextra)
install.packages("factoextra")
install.packages("factoextra")
install.packages("factoextra")
library(factoextra)
package_version()
install.packages("factoextra")
library(factoextra)
iris
data_omit <- na.omit(iris)
data_omit
data_omit_scale <- scale(data_omit)
data_omit_scale <- scale("data_omit")
data_omit_scale
data_omit_scale <- scale(,data_omit)
data_omit_scale <- scale(data_omit,)
data_omit_scale <- scale(1,data_omit)
data_omit_scale <- scale(1)
data_omit_scale
data_omit_scale <- scale(5)
data_omit_scale
data_subset <- data_omit_scale[1:5]
data_subset
fviz_nbclust(data_subset,kmeans(),method = "wss")+labs(subtitle = "Elbow Method")
setwd("C:/tmp/datamining")
getwd()
library(readr)
library(dplyr)
library(ggplot2)
train_data <- read.csv("myCarTrainDataset_2023.csv")
head(train_data)
summary(train_data)
str(train_data)
dim(train_data)
set.seed(12)
sample(1:nrow(train_data),10,replace = FALSE)
names(train_data)
str(train_data)
table(train_data)
quantile(train_data)
quantile(train_data$buying)
quantile(train_data$maint)
quantile(train_data$persons)
quantile(train_data$safety)
quantile(train_data$acceptance)
quantile(train_data$persons)
quantile(train_data$persons,c(.12,.44,.68))
max(train_data)
max(train_data$buying)
max(train_data$buying)-min(train_data$buying)
max(train_data$buying)-min(train_data$maint)
max(train_data$buying)-min(train_data$persons)
max(train_data$buying)-min(train_data$safety)
max(train_data$persons)-min(train_data$persons)
max(train_data$persons)-min(train_data$persons)
mean(train_data$persons)
var(train_data$persons)
sd(train_data$persons)
summary(train_data$persons)
summary(train_data$buying)
summary(train_data)
cor(train_data[,1:4])
cor(train_data$persons[,1:4])
cor(train_data$persons[,1:5])
cor(train_data$persons[,1:1])
cor(train_data$persons)
boxplot(train_data[, c("buying", "maint", "persons", "safety")])
# Create a new data frame with separate columns for each numerical variable
data_numeric <- train_data[, c("buying", "maint", "persons", "safety")]
# Create a list of titles for the boxplots
boxplot_titles <- c("Buying Price", "Maintenance Price", "Car Capacity", "Safety")
# Create a grid of boxplots using ggplot2
p <- ggplot()
# Add each boxplot to the grid
for (i in 1:4) {
p <- p + ggplot(data_numeric, aes(x = ~i)) + geom_boxplot(aes(color = acceptance)) +
facet_wrap(~variable) + labs(title = boxplot_titles[i], x = NULL, y = NULL)
}
rlang::last_trace()
rlang::last_trace(drop = FALSE)
ggplot(train_data, aes(x = buying, y = safety)) +
geom_point() +
labs(title = "Scatter Plot of Buying Price vs. Safety",
x = "Buying Price",
y = "Safety")
# Assuming 'buying' and 'safety' are numeric variables, and 'acceptance' is a factor
ggplot(train_data, aes(x = buying, y = safety, color = acceptance)) +
geom_point() +
labs(title = "Colored Scatter Plot of Buying Price vs. Safety",
x = "Buying Price",
y = "Safety",
color = "Acceptance")
ggplot(myCarTrainDataset, aes(x = buying, y = safety, color = acceptance, shape = acceptance)) +
geom_point(size = 3) +
labs(title = "Scatter Plot of Buying Price vs. Safety",
x = "Buying Price",
y = "Safety",
color = "Acceptance",
shape = "Acceptance") +
scale_color_manual(values = c("green", "blue", "red")) +  # Set custom colors
scale_shape_manual(values = c(16, 17, 18)) +  # Set custom shapes
theme_minimal()  # Customize the theme if needed
ggplot(myCarTrainDataset_2023, aes(x = buying, y = safety, color = acceptance, shape = acceptance)) +
geom_point(size = 3) +
labs(title = "Scatter Plot of Buying Price vs. Safety",
x = "Buying Price",
y = "Safety",
color = "Acceptance",
shape = "Acceptance") +
scale_color_manual(values = c("green", "blue", "red")) +  # Set custom colors
scale_shape_manual(values = c(16, 17, 18)) +  # Set custom shapes
theme_minimal()  # Customize the theme if needed
ggplot(train_data, aes(x = buying, y = safety, color = acceptance, shape = acceptance)) +
geom_point(size = 3) +
labs(title = "Scatter Plot of Buying Price vs. Safety",
x = "Buying Price",
y = "Safety",
color = "Acceptance",
shape = "Acceptance") +
scale_color_manual(values = c("green", "blue", "red")) +  # Set custom colors
scale_shape_manual(values = c(16, 17, 18)) +  # Set custom shapes
theme_minimal()  # Customize the theme if needed
ggplot(train_data, aes(x = buying, y = safety, color = acceptance, shape = acceptance)) +
geom_point(size = 3) +
labs(title = "Scatter Plot of Buying Price vs. Safety",
x = "Buying Price",
y = "Safety",
color = "Acceptance",
shape = "Acceptance") +
scale_color_manual(values = c("green", "blue", "red")) +  # Set custom colors
scale_shape_manual(values = c(16, 17, 18)) +  # Set custom shapes
theme_minimal()+  # Customize the theme if needed
theme(legend.position = "top")  # Adjust legend position if needed
ggplot(train_data, aes(x = buying, y = safety, color = acceptance, shape = acceptance)) +
geom_point(size = 3) +
labs(title = "Scatter Plot of Buying Price vs. Safety",
x = "Buying Price",
y = "Safety",
color = "Acceptance",
shape = "Acceptance") +
scale_color_manual(values = c("green", "blue", "red")) +  # Set custom colors
scale_shape_manual(values = c(16, 17, 18)) +  # Set custom shapes
theme_minimal()+ # Customize the theme if needed
theme(legend.position = "top")  # Adjust legend position if needed
theme(legend.position = "top")  # Adjust legend position if needed
R version 4.3.1 (2023-06-16 ucrt) -- "Beagle Scouts"
R version 4.3.1 (2023-06-16 ucrt) -- "Beagle Scouts"
install.packages("scatterplot3d")
library(scatterplot3d)
# Assuming 'buying', 'maint', 'safety' are numeric variables, and 'acceptance' is a factor
# Replace them with the actual column names from your dataset
scatterplot3d(myCarTrainDataset[, c('buying', 'maint', 'safety')],
color = rainbow(3)[match(myCarTrainDataset$acceptance, levels(myCarTrainDataset$acceptance))],
pch = 16,
main = "3D Scatter Plot for Car Dataset",
xlab = "Buying Price",
ylab = "Maintenance Price",
zlab = "Safety")
# Assuming 'buying', 'maint', 'safety' are numeric variables, and 'acceptance' is a factor
# Replace them with the actual column names from your dataset
scatterplot3d(train_data[, c('buying', 'maint', 'safety')],
color = rainbow(3)[match(train_data$acceptance, levels(train_data$acceptance))],
pch = 16,
main = "3D Scatter Plot for Car Dataset",
xlab = "Buying Price",
ylab = "Maintenance Price",
zlab = "Safety")
# Add legend
legend("topright", legend = levels(train_data$acceptance), fill = rainbow(3), title = "Acceptance")
# Assuming 'buying', 'maint', 'safety' are numeric variables, and 'acceptance' is a factor
# Replace them with the actual column names from your dataset
scatterplot3d(train_data[, c('buying', 'maint', 'safety')],
color = rainbow(3)[match(train_data$acceptance, levels(train_data$acceptance))],
pch = 16,
main = "3D Scatter Plot for Car Dataset",
xlab = "Buying Price",
ylab = "Maintenance Price",
zlab = "Safety")
# Add legend
legend("topright", legend = levels(train_data$acceptance), fill = rainbow(3), title = "Acceptance")
# Assuming 'buying', 'maint', 'safety' are numeric variables, and 'acceptance' is a factor
# Replace them with the actual column names from your dataset
scatterplot3d(train_data[, c('buying', 'maint', 'safety')],
color = rainbow(3)[match(train_data$acceptance, levels(train_data$acceptance))],
pch = 16,
main = "3D Scatter Plot for Car Dataset",
xlab = "Buying Price",
ylab = "Maintenance Price",
zlab = "Safety")
unique(train_data$acceptance)
train_data$acceptance <- as.factor(train_data$acceptance)
# Assuming 'acceptance' is a factor with multiple levels
legend("topright", legend = levels(train_data$acceptance), fill = rainbow(length(levels(train_data$acceptance))), title = "Acceptance")
scatterplot3d(train_data[, c('buying', 'maint', 'safety')],
+               color = rainbow(3)[match(train_data$acceptance, levels(train_data$acceptance))],
scatterplot3d(train_data[, c('buying', 'maint', 'safety')],
color = rainbow(3)[match(train_data$acceptance, levels(train_data$acceptance))],
pch = 16,
main = "3D Scatter Plot for Car Dataset",
xlab = "Buying Price",
ylab = "Maintenance Price",
zlab = "Safety")
# Add legend
# Assuming 'acceptance' is a factor with multiple levels
legend("topright", legend = levels(train_data$acceptance), fill = rainbow(length(levels(train_data$acceptance))), title = "Acceptance")
#boxplot
# Assuming 'acceptance' is a factor variable
boxplot(safety ~ acceptance, data = train_data, col = rainbow(length(levels(train_data$acceptance))),
main = "Boxplot of Safety by Acceptance",
xlab = "Acceptance",
ylab = "Safety")
# Convert 'safety' to numeric
train_data$safety <- as.numeric(as.character(train_data$safety))
# Create the boxplot
library(ggplot2)
ggplot(data, aes(x = acceptance, y = safety)) +
geom_boxplot() +
labs(title = "Boxplot of Safety by Acceptance", x = "Acceptance", y = "Safety")
# Create the boxplot
library(ggplot2)
ggplot(train_data, aes(x = acceptance, y = safety)) +
geom_boxplot() +
labs(title = "Boxplot of Safety by Acceptance", x = "Acceptance", y = "Safety")
boxplot(train_data[,1:4], main = "Car data box plots for columns")
boxplot(train_data, main = "Car data box plots for columns")
# Assuming 'buying', 'maint', 'persons', 'safety' are numeric variables
boxplot(train_data[, c('buying', 'maint', 'persons', 'safety')],
main = "Car Training Dataset Box Plots for Numeric Variables",
col = rainbow(4),
xlab = "Variables",
ylab = "Values")
# Check data types of relevant columns
sapply(train_data[, c('buying', 'maint', 'persons', 'safety')], class)
# Convert 'safety' to numeric
train_data$safety <- as.numeric(train_data$safety)
# Create boxplot
boxplot(train_data[, c('buying', 'maint', 'persons', 'safety')],
main = "Car Training Dataset Box Plots for Numeric Variables",
col = rainbow(4),
xlab = "Variables",
ylab = "Values")
train_data$safety <- as.numeric(as.character(train_data$safety))
sapply(train_data[, c('buying', 'maint', 'persons', 'safety')], function(x) any(!is.numeric(x)))
table(train_data$safety)
# Assuming 'buying', 'maint', 'persons', 'safety' are numeric variables
par(mfrow = c(2, 2))  # Set the layout to a 2x2 grid
hist(train_data$buying, col = "skyblue", main = "Buying Prices", xlab = "Buying Price", ylab = "Frequency")
# Assuming 'buying', 'maint', 'persons', 'safety' are numeric variables
par(mfrow = c(2, 2))  # Set the layout to a 2x2 grid
hist(train_data$buying, col = "skyblue", main = "Buying Prices", xlab = "Buying Price", ylab = "Frequency")
# Assuming 'buying', 'maint', 'persons', 'safety' are numeric variables
par(mfrow = c(2, 2))  # Set the layout to a 2x2 grid
hist(train_data$maint, col = "lightgreen", main = "Maintenance Prices", xlab = "Maintenance Price", ylab = "Frequency")
# Assuming 'buying', 'maint', 'persons', 'safety' are numeric variables
par(mfrow = c(2, 2))  # Set the layout to a 2x2 grid
hist(train_data$persons, col = "salmon", main = "Car Capacity", xlab = "Car Capacity", ylab = "Frequency")
hist(train_data$safety, col = "gold", main = "Estimated Safety", xlab = "Estimated Safety", ylab = "Frequency")
# Assuming 'buying', 'maint', 'persons', 'safety' are numeric variables
par(mfrow = c(2, 2))  # Set the layout to a 2x2 grid
hist(train_data$persons, col = "salmon", main = "Car Capacity", xlab = "Car Capacity", ylab = "Frequency")
par(mfrow = c(1, 1))  # Reset the layout to the default
install.packages("plot3D")
# Install and load the necessary library
install.packages("rgl")
library(rgl)
# Assuming 'buying', 'maint', 'safety' are numeric variables
# Replace them with the actual column names from your dataset
# Create a 3D histogram plot
plot3d(train_data$buying, train_data$maint, train_data$safety,
col = "skyblue",
xlab = "Buying Price",
ylab = "Maintenance Price",
zlab = "Safety",
main = "3D Histogram Plot for Car Training Dataset")
# Add grid for better visualization
grid3d("box")
# Install and load the necessary library
install.packages("rpart")
library(rpart)
# Assuming 'buying', 'maint', 'persons', and 'safety' are predictor variables
# and 'acceptance' is the target variable
# Replace them with the actual column names from your dataset
# Create a decision tree using the Hunt Algorithm
tree_model <- rpart(acceptance ~ buying + maint + persons + safety,
data = train_data,
method = "class")  # For classification problems
# Print the decision tree
print(tree_model)
# Plot the decision tree (requires 'rpart.plot' package)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree_model, main = "Decision Tree for Car Acceptance")
# Assuming 'buying', 'maint', 'persons', 'safety', and 'acceptance' are columns in your dataset
# Replace them with the actual column names
# Sample car training dataset
train_data <- read.csv("myCarTrainDataset.csv")  # Update the file path accordingly
# Assuming 'buying', 'maint', 'persons', 'safety', and 'acceptance' are columns in your dataset
# Replace them with the actual column names
# Sample car training dataset
train_data <- read.csv("myCarTrainDataset.csv")  # Update the file path accordingly
# Assuming 'buying', 'maint', 'persons', 'safety', and 'acceptance' are columns in your dataset
# Replace them with the actual column names
# Sample car training dataset
train_data <- read.csv("myCarTrainDataset_2023.csv")  # Update the file path accordingly
# Function to calculate Gini index for a node
calculateGini <- function(data) {
classProportions <- prop.table(table(data$acceptance))
gini <- 1 - sum(classProportions^2)
return(gini)
}
# Function to calculate Gini impurity for a split
calculateGiniImpurity <- function(data, splitVar, splitValue) {
leftData <- data[data[[splitVar]] <= splitValue, ]
rightData <- data[data[[splitVar]] > splitValue, ]
giniParent <- calculateGini(data)
giniLeft <- calculateGini(leftData)
giniRight <- calculateGini(rightData)
nTotal <- nrow(data)
nLeft <- nrow(leftData)
nRight <- nrow(rightData)
impurity <- giniParent - (nLeft/nTotal * giniLeft + nRight/nTotal * giniRight)
return(impurity)
}
# Example usage
giniIndex <- calculateGini(train_data)
cat("Gini Index for the entire dataset:", giniIndex, "\n")
# Assuming you split the data into leftData and rightData based on a certain variable and value
splitVar <- "buying"
splitValue <- 2  # Update with a suitable threshold value
giniImpurity <- calculateGiniImpurity(train_data, splitVar, splitValue)
cat("Gini Impurity for the split:", giniImpurity, "\n")
# Sample car training dataset
train_data <- read.csv("myCarTrainDataset_2023.csv")  # Update the file path accordingly
# Function to calculate Gini index for a node
calculateGini <- function(data) {
classProportions <- prop.table(table(data$acceptance))
gini <- 1 - sum(classProportions^2)
return(gini)
}
# Function to calculate Gini impurity for a split
calculateGiniImpurity <- function(data, splitVar, splitValue) {
leftData <- data[data[[splitVar]] <= splitValue, ]
rightData <- data[data[[splitVar]] > splitValue, ]
giniParent <- calculateGini(data)
giniLeft <- calculateGini(leftData)
giniRight <- calculateGini(rightData)
nTotal <- nrow(data)
nLeft <- nrow(leftData)
nRight <- nrow(rightData)
impurity <- giniParent - (nLeft/nTotal * giniLeft + nRight/nTotal * giniRight)
return(impurity)
}
# Function to find the best split for a variable
findBestSplit <- function(data, splitVar) {
unique_values <- unique(data[[splitVar]])
best_split_value <- NULL
best_impurity <- Inf
for (value in unique_values) {
impurity <- calculateGiniImpurity(data, splitVar, value)
if (impurity < best_impurity) {
best_impurity <- impurity
best_split_value <- value
}
}
return(list(split_value = best_split_value, impurity = best_impurity))
}
# Example usage for the 'buying' variable
best_split <- findBestSplit(train_data, "buying")
cat("Best Split Value for 'buying':", best_split$split_value, "\n")
cat("Gini Impurity for the Best Split:", best_split$impurity, "\n")
# Install and load necessary libraries
install.packages("rpart")
library(rpart)
# Sample car training dataset
train_data <- read.csv("myCarTrainDataset_2023.csv")  # Update the file path accordingly
# Assuming 'buying', 'maint', 'persons', and 'safety' are predictor variables
# and 'acceptance' is the target variable
# Replace them with the actual column names
# Create a decision tree using the Hunt Algorithm with Gini impurity
tree_model <- rpart(acceptance ~ buying + maint + persons + safety,
data = train_data,
method = "class",  # For classification problems
parms = list(split = "gini"))  # Use Gini impurity
# Print the decision tree
print(tree_model)
# Plot the decision tree (requires 'rpart.plot' package)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree_model, main = "Decision Tree with Gini Impurity for Car Acceptance")
install.packages("rpart")
install.packages("rpart")
install.packages("rpart.plot")
# Install and load necessary libraries
install.packages(c("rpart", "rpart.plot"))
library(rpart)
library(rpart.plot)
# Sample car training dataset
train_data <- read.csv("myCarTrainDataset_2023.csv")  # Update the file path accordingly
# Assuming 'buying', 'maint', 'persons', and 'safety' are predictor variables
# and 'acceptance' is the target variable
# Replace them with the actual column names
# Convert 'buying' into dummy variables
train_data_dummies <- model.matrix(~ buying - 1, data = train_data)
train_data_dummies <- as.data.frame(train_data_dummies)
# Combine the dummy variables with the original dataset
train_data <- cbind(train_data, train_data_dummies)
# Create a decision tree using the Hunt Algorithm with Gini impurity and multiway splits
tree_model <- rpart(acceptance ~ buying.high + buying.low + buying.med + buying.vhigh + maint + persons + safety,
data = train_data,
method = "class",  # For classification problems
parms = list(split = "gini"))  # Use Gini impurity
install.packages(c("rpart", "rpart.plot"))
# Install and load necessary libraries
install.packages(c("rpart", "rpart.plot"))
library(rpart)
library(rpart.plot)
# Sample car training dataset
train_data <- read.csv("myCarTrainDataset_2023.csv")  # Update the file path accordingly
# Assuming 'buying', 'maint', 'persons', and 'safety' are predictor variables
# and 'acceptance' is the target variable
# Replace them with the actual column names
# Convert 'buying' into dummy variables
train_data_dummies <- model.matrix(~ buying - 1, data = train_data)
train_data_dummies <- as.data.frame(train_data_dummies)
# Combine the dummy variables with the original dataset
train_data <- cbind(train_data, train_data_dummies)
# Create a decision tree using the Hunt Algorithm with Gini impurity and multiway splits
tree_model <- rpart(acceptance ~ buying.high + buying.low + buying.med + buying.vhigh + maint + persons + safety,
data = train_data,
method = "class",  # For classification problems
parms = list(split = "gini"))  # Use Gini impurity
install.packages(c("rpart", "rpart.plot"))
install.packages(c("rpart", "rpart.plot"))
install.packages(c("rpart", "rpart.plot"))
# Assuming you have already built the fully-grown decision tree (tree_model)
# from the previous steps
# Load necessary libraries
install.packages(c("caret", "e1071"))
library(caret)
library(e1071)
# Sample car test dataset
test_data <- read.csv("myCarTestDataset_2023.csv")  # Update the file path accordingly
# Assuming 'buying', 'maint', 'persons', and 'safety' are predictor variables
# and 'acceptance' is the target variable
# Replace them with the actual column names
# Convert 'buying' into dummy variables
test_data_dummies <- model.matrix(~ buying - 1, data = test_data)
test_data_dummies <- as.data.frame(test_data_dummies)
# Combine the dummy variables with the original dataset
test_data <- cbind(test_data, test_data_dummies)
# Apply post-pruning to the decision tree
pruned_tree_model <- prune(tree_model, cp = 0.01)  # Adjust the cp value as needed
# Assuming you have already built the fully-grown decision tree (tree_model)
# from the previous steps
# Load necessary libraries
install.packages(c("caret", "e1071"))
library(caret)
library(e1071)
# Sample car test dataset
test_data <- read.csv("myCarTestDataset_2023.csv")  # Update the file path accordingly
# Assuming 'buying', 'maint', 'persons', and 'safety' are predictor variables
# and 'acceptance' is the target variable
# Replace them with the actual column names
# Convert 'buying' into dummy variables
test_data_dummies <- model.matrix(~ buying - 1, data = test_data)
test_data_dummies <- as.data.frame(test_data_dummies)
# Combine the dummy variables with the original dataset
test_data <- cbind(test_data, test_data_dummies)
# Apply post-pruning to the decision tree
pruned_tree_model <- prune(tree_model, cp = 0.01)  # Adjust the cp value as needed
install.packages(c("caret", "e1071"))
